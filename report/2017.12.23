1.神经网络中l层的激活值a，权重w，偏置b的关系公式：a^l= σ(w^l∙a^(l-1) + b^l  )其中z^l=w^l∙a^(l-1)   + b^l，z称为l层的带权输入，注：^表示上标
2.反向传播实际上是对⼀个独⽴的训练样本计算了 ∂Cx/∂w 和 ∂Cx/∂b。然后我们通过在所有训练样本上进行平均化获得 ∂C/∂w 和 ∂C/∂b。
3. Hadamard 乘积(又称Schur 乘积)：使⽤ s ⊙ t 来表⽰按元素的乘积。所以 S⊙T 的元素就是 (S⊙T) j = SjTj 。给个例⼦，S=[1,2] T=[3,4],S⊙T=[3,8]
4.引入一个中间量，δl(上标）j（下标） ，这个我们称为在 l th 层第 j th 个神经元上的误差，δ(lj)≡∂C/∂z(lj)
5.输出层误差方程：δ(Lj)=∂C/∂a(Lj)*σ′(z(Lj))，δ（L） = ∇aC⊙σ′(z(L))
6.使用下一层的误差 δ(l+1)来表⽰当前层的误差 δ(l):δ(l)= ((w (l+1) ) T δ l+1 ) ⊙ σ ′ (z l ) 
7.误差 δ lj和偏导数值 ∂C/∂b lj完全⼀致（已自证）
8.∂C/∂w(ljk)= a(l−1k)*δ(lj)（已自证）
